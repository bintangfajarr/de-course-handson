
#  E-Commerce Data Engineering Pipeline  
**Prefect â€¢ AWS S3 â€¢ Docker â€¢ Python**

---

##  Project Overview
This project implements an **end-to-end batch Data Engineering pipeline** for e-commerce data using **Prefect for orchestration** and **AWS S3 as a data lake**.

The pipeline is designed with **production-oriented architecture**, covering:
- Raw data ingestion
- Data cleaning & transformation
- Business metrics generation
- Cloud storage integration
- Workflow orchestration
- Containerized execution

---

## Flow

```

CSV Data Sources
â”‚
â–¼
AWS S3 (Raw Zone)
raw-data/
â”‚
â–¼
Prefect Orchestration
ecommerce_etl_pipeline
â”‚
â–¼
Data Processing Layer
(Pandas Transformations)
â”‚
â–¼
Business Metrics Layer
â”‚
â–¼
AWS S3 (Processed Zone)
processed/
â””â”€â”€ metrics/

```

---

##  Architecture Layers

### ðŸ”¹ Data Source Layer
- CSV files representing e-commerce entities:
  - customers
  - products
  - orders
  - order_items
  - reviews

---

### ðŸ”¹ Data Lake Layer (AWS S3)
The project follows a **simplified Medallion Architecture**:

| Layer | S3 Path | Description |
|-----|--------|------------|
| Raw | `raw-data/` | Immutable raw data |
| Processed | `processed/` | Cleaned & enriched datasets |
| Metrics | `processed/metrics/` | Business-ready analytics |

---

### ðŸ”¹ Orchestration Layer (Prefect)
- Manages task dependencies
- Automatic retry & failure handling
- Centralized logging and observability
- Supports local and containerized execution

---

### ðŸ”¹ Processing Layer
- Data cleaning and normalization
- Feature engineering
- Time-based transformations
- Schema enrichment

---

### ðŸ”¹ Business Metrics Layer
This layer generates analytics-ready datasets:
- Customer Lifetime Value
- Product performance metrics
- Monthly sales trends

---

##  Project Structure

```

data-pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ aws_config.yaml
â”‚   â””â”€â”€ prefect_config.yaml
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ docker/
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”‚   â””â”€â”€ s3_uploader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â””â”€â”€ data_processing.py
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â””â”€â”€ prefect_flow.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ prefect_test.py
â”‚   â””â”€â”€ test_aws_connection.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ prefect.yaml
â””â”€â”€ README.md

````

---

##  Pipeline Flow
1. Download raw data from AWS S3
2. Clean and transform datasets
3. Generate business metrics
4. Upload processed data back to S3
5. Orchestrate execution using Prefect

---

##  Tech Stack

| Component | Technology |
|---------|-----------|
| Language | Python |
| Orchestration | Prefect |
| Storage | AWS S3 |
| Processing | Pandas |
| Containerization | Docker |

---

##  Setup & Installation

### Clone Repository
```bash
git clone https://github.com/bintangfajarr/de-course-handson.git
cd de-course-handson/data-pipeline
````

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

##  Environment Variables

Create a `.env` file:

```env
AWS_S3_BUCKET_NAME=your-bucket-name
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
```

---

##  Running the Pipeline

### Start Prefect Server

```bash
prefect server start
```

### Execute the Flow

```bash
python src/orchestration/prefect_flow.py
```

---

##  Run with Docker

```bash
docker-compose up
```

---

##  Key Features

* Modular ETL pipeline design
* Fault-tolerant task orchestration
* Business metrics generation
* Cloud-native data lake architecture
* Production-ready project structure

---

##  Future Enhancements

* Data quality validation
* Spark-based large-scale processing
* Data warehouse integration (Redshift / BigQuery)
* CI/CD automation
* dbt metrics layer
---


